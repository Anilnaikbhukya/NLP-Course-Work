{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M3r84BT5WeN"
   },
   "source": [
    "#955G5: Applied Natural Language Processing\n",
    "\n",
    "##Computer Based Examination, 2023\n",
    "\n",
    "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "259FEqEb5xRr"
   },
   "source": [
    "\n",
    "\n",
    "#Question 3 (50 Marks)\n",
    "\n",
    "This question is about Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "89Wx4WPa5weU"
   },
   "outputs": [],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "import nltk\n",
    "\n",
    "# This is a list of sentences with NER tags which we will use in this question.\n",
    "tagged_sents=[[\"tim_PER\", \"cook_PER\", \"is\", \"the\", \"current\", \"ceo\", \"of\", \"apple_ORG\", \"inc._ORG\", \",\", \"headquartered\", \"in\", \"cupertino_LOC\", \".\"],\n",
    "              [\"fiona_PER\", \"apple_PER\", \"was\", \"born\", \"in\", \"new_LOC\", \"york_LOC\", \"and\", \"her\", \"debut\", \"album\", \"was\", \"released\", \"by\", \"columbia_ORG\", \"records_ORG\", \".\"],\n",
    "              [\"the_LOC\", \"big_LOC\", \"apple_LOC\", \"is\", \"a\", \"nickname\", \"for\", \"new_LOC\", \"york_LOC\", \"city_LOC\", \".\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MUMcHxbBsBi"
   },
   "source": [
    "a) Follow the steps below to preprocess the NER tagged sentences, producing a list of lists of tokens and a list of lists of NER tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FDwzz0oCL8P"
   },
   "source": [
    "i) Use the `.split()` method to separate the tokens from the NER tags in the list `tagged_sents`, and strip off the tags to produce a list of lists of strings, called `tokens`, corresponding to just the tokens in each sentence.\n",
    "\n",
    "So, for example `[[\"Alice_PER\", \"runs\"], [\"Bob_PER\", \"walks\"]]` would produce `[[\"Alice\", \"runs\"], [\"Bob\", \"walks\"]]`.\n",
    "\n",
    "(8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tim', 'cook', 'is', 'the', 'current', 'ceo', 'of', 'apple', 'inc.', ',', 'headquartered', 'in', 'cupertino', '.', 'fiona', 'apple', 'was', 'born', 'in', 'new', 'york', 'and', 'her', 'debut', 'album', 'was', 'released', 'by', 'columbia', 'records', '.', 'the', 'big', 'apple', 'is', 'a', 'nickname', 'for', 'new', 'york', 'city', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for sentence in tagged_sents:\n",
    "    for word in sentence:\n",
    "        tokens.append(word.split('_')[0])\n",
    "print(tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glog6pY1NkcV"
   },
   "source": [
    "ii) Use the `.split()` method to separate the tokens from the NER tags in the list `tagged_sents`, producing a list of lists of NER tags, called `tags`. Tokens without an NER tag should be tagged with the letter `\"O\"`.\n",
    "\n",
    "So, for example `[[\"Alice_PER\", \"runs\"], [\"chase\", \"Bob_PER\"]]` would produce `[[\"PER\", \"O\"], [\"O\", \"PER\"]]`.\n",
    "\n",
    "(6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'O', 'O', 'O', 'LOC', 'O', 'PER', 'PER', 'O', 'O', 'O', 'LOC', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'O', 'LOC', 'LOC', 'LOC', 'O', 'O', 'O', 'O', 'LOC', 'LOC', 'LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "\n",
    "for sentence in tagged_sents:\n",
    "    for word in sentence:\n",
    "        if '_' in word:\n",
    "            tags.append(word.split('_')[1])\n",
    "        else:\n",
    "            tags.append('O')\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhzmSCQTRhJo"
   },
   "source": [
    "b) Now, follow the steps below to derive the transition and emission probabilities of a Hidden Markov Model for the NER tag sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lz2eIJ6Ry6w"
   },
   "source": [
    "i) Describe the two assumptions that a Hidden Markov Model for sequence tagging is based on.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2vWgcgIUnvp"
   },
   "source": [
    "A Hidden Markov Model (HMM) for sequence tagging is based on two fundamental concepts:\n",
    "\n",
    "1. **The Markov Process Hypothesis:** \n",
    "+ Think of it as a chain of events in which the probability of a specific situation (such as a specific named entity tag) at any given time is determined solely by what came before it. So the current situation is entirely concerned with what has just occurred; it is unconcerned with everything that has preceded it. This makes things easier because we don't have to remember a slew of previous events, only the one right before it.\n",
    "\n",
    "+ It's the mathematical equivalent of saying, \"What's happening now (q_i) depends only on what happened before (q_i-1).\" In terms of computation, this keeps the model neat and tidy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **Assumption of Output Independence:**\n",
    "\n",
    "+ Consider each observation in isolation (as if it were a single word in a sentence). This assumption implies that what you're seeing now is only relevant to what's happening now. It is unconcerned about what has occurred or what may occur in the future. It simplifies things by saying, \"What I'm seeing (o_i) depends only on the situation I'm in right now (q_i).\"\n",
    "\n",
    "\n",
    "+ This is equivalent to saying, \"What I'm observing right now (o_i) is all about the current situation (q_i).\" This allows the model to zero in on the likelihood of seeing a specific word in a specific state (such as a named entity tag).\n",
    "\n",
    "\n",
    "\n",
    "These ideas combine to make the HMM a useful tool for tagging sequences. The Markov assumption allows us to use clever decoding algorithms, such as the Viterbi algorithm, and the output independence assumption makes calculating the likelihood of seeing a specific word in a given situation easier. It's like breaking a complicated story down into manageable bite-sized chunks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDlCGNMdSHBx"
   },
   "source": [
    "ii) Create a dictionary, called `nercounts`, containing the total counts for each tag in the list `tags`. The keys of `nercounts` should be tags and the values should be the frequencies of those tags.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASbaMzO74wMg",
    "outputId": "00a20586-90c1-44dd-ca24-6441166ce849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PER': 2, 'O': 7, 'ORG': 1, 'LOC': 2}\n"
     ]
    }
   ],
   "source": [
    "tags = [\"PER\", \"O\", \"O\", \"ORG\", \"LOC\", \"PER\", \"O\", \"O\", \"O\", \"O\", \"LOC\", \"O\"]\n",
    "\n",
    "\n",
    "nercounts = {}\n",
    "for tag in tags:\n",
    "    if tag in nercounts:\n",
    "        nercounts[tag] += 1\n",
    "    else:\n",
    "        nercounts[tag] = 1\n",
    "\n",
    "\n",
    "print(nercounts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ54_g706lpv"
   },
   "source": [
    "iii) Using the lists `tags` and `tokens`, calculate the emission probabilities: $$p(token|tag)$$\n",
    "\n",
    "Put these probabilities in a dictionary of dictionaries, called `emission`, with outer keys being NER tags, inner keys being tokens and inner values being the emission probabilities.\n",
    "\n",
    "(8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ttpe2z36R2n0",
    "outputId": "05313a53-782c-4fa7-a0f2-15038d6ede37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PER': {'tim': 0.5, 'ceo': 0.5}, 'O': {'cook': 0.14285714285714285, 'is': 0.14285714285714285, 'of': 0.14285714285714285, 'apple': 0.14285714285714285, 'inc.': 0.14285714285714285, ',': 0.14285714285714285, 'in': 0.14285714285714285}, 'ORG': {'the': 1.0}, 'LOC': {'current': 0.5, 'headquartered': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "tags = [\"PER\", \"O\", \"O\", \"ORG\", \"LOC\", \"PER\", \"O\", \"O\", \"O\", \"O\", \"LOC\", \"O\"]\n",
    "tokens = [\"tim\", \"cook\", \"is\", \"the\", \"current\", \"ceo\", \"of\", \"apple\", \"inc.\", \",\", \"headquartered\", \"in\", \"cupertino\", \".\"]\n",
    "\n",
    "\n",
    "emission = {}\n",
    "\n",
    "for tag, token in zip(tags, tokens):\n",
    "    if tag not in emission:\n",
    "        emission[tag] = {}\n",
    "\n",
    "    if token in emission[tag]:\n",
    "        emission[tag][token] += 1\n",
    "    else:\n",
    "        emission[tag][token] = 1\n",
    "\n",
    "\n",
    "for tag in emission:\n",
    "    total_count = sum(emission[tag].values())\n",
    "    for token in emission[tag]:\n",
    "        emission[tag][token] /= total_count\n",
    "\n",
    "\n",
    "print(emission)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugDBChyQ8MHu"
   },
   "source": [
    "iv) Using the list `tags`, calculate the transition probabilities: $$p(tag_i|tag_{i-1})$$\n",
    "\n",
    "You will need to keep track of both the current tag, $tag_i$, and the previous tag, $tag_{i-1}$, and you will need to introduce a special tag, e.g. \"START\", for the previous tag at the beginning of a sequence.\n",
    "\n",
    "For example, the sequence of tags `[\"PER\", \"O\"]` would give rise to the following values of $i$, $tag_{i-1}$ and $tag_i$:\n",
    "\n",
    "|i|$tag_{i-1}$|$tag_i$|\n",
    "|---|---|---|\n",
    "|0|START|PER|\n",
    "|1|PER|O|\n",
    "\n",
    "Calculate the conditional probabilities of $tag_i$ given $tag_{i-1}$ and put the results in a dictionary of dictionaries, called `transition`, with outer keys being previous tags, inner keys being current tags and inner values being transition probabilities.\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbRg_hyU_76c",
    "outputId": "66e110ea-f6cb-49c4-cd1b-9556a4295b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'START': {'PER': 1.0}, 'PER': {'O': 1.0}, 'O': {'O': 0.6666666666666666, 'ORG': 0.16666666666666666, 'LOC': 0.16666666666666666}, 'ORG': {'LOC': 1.0}, 'LOC': {'PER': 0.5, 'O': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "tags = [\"START\", \"PER\", \"O\", \"O\", \"ORG\", \"LOC\", \"PER\", \"O\", \"O\", \"O\", \"O\", \"LOC\", \"O\"]\n",
    "\n",
    "\n",
    "transition = {}\n",
    "\n",
    "for i in range(1, len(tags)):\n",
    "    current_tag = tags[i]\n",
    "    previous_tag = tags[i - 1]\n",
    "\n",
    "    if previous_tag not in transition:\n",
    "        transition[previous_tag] = {}\n",
    "\n",
    "    if current_tag in transition[previous_tag]:\n",
    "        transition[previous_tag][current_tag] += 1\n",
    "    else:\n",
    "        transition[previous_tag][current_tag] = 1\n",
    "\n",
    "\n",
    "for previous_tag in transition:\n",
    "    total_count = sum(transition[previous_tag].values())\n",
    "    for current_tag in transition[previous_tag]:\n",
    "        transition[previous_tag][current_tag] /= total_count\n",
    "\n",
    "\n",
    "print(transition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDevraTjRH5X"
   },
   "source": [
    "v) Given a sequence of tokens, we want to find the most probable sequence of tags corresponding to those tokens. The brute force approach would simply evaluate the probability of every sequence of tags. Explain why we would want to avoid that approach and describe an alternative algorithm.\n",
    "\n",
    "(6 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7YwQI7MZxEY"
   },
   "source": [
    "The brute force approach of evaluating the probability of every sequence of tags is computationally expensive and impractical for several reasons:\n",
    "\n",
    "1. **Exponential Complexity:**\n",
    "   + The brute force approach of evaluating the probability of each tag sequence becomes impractical due to the exponential growth in possibilities with sequence length.\n",
    "   + The computational cost of considering all combinations makes this approach impractical for long sequences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **High Memory Usage:**\n",
    "   + Large amounts of memory are required to store and compute probabilities for all possible tag sequences.\n",
    "   + As the sequence length increases, the memory usage becomes unmanageable, making it unsuitable for real-world applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. **Inefficiency in Search Space:**\n",
    "   + The brute force approach is inefficient because not all tag sequences contribute significantly to the final probability.\n",
    "   + Many sequences are unlikely and can be ignored, highlighting the importance of a more focused algorithm.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Alternative: Viterbi Algorithm**\n",
    "   + The Viterbi Algorithm is a quick way to find the most likely tag sequence.\n",
    "   + Based on the Markov assumption, it uses dynamic programming to determine the most likely path to each state at each time step.\n",
    "   + The algorithm is a more practical and scalable choice for sequence tagging tasks due to its linear time complexity in terms of the product of states and sequence length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdxdMimsLwpx"
   },
   "source": [
    "vi) If we are tagging a sequence and the current token is `\"apple\"` and the previous tag is `\"LOC\"`, then we want know to which tag in the current position  would maximise the probability of seeing that token.\n",
    "\n",
    "Define a function that takes a token, a previous tag, a dictionary of emission probabilities and a dictionary of transition probabilities and returns a dictionary of the probabilities $p(token, tag_i | tag_{i-1})$ for all values of $tag_i$.\n",
    "\n",
    "Apply this function to the case where the current token is `\"apple\"` and the previous tag is `\"LOC\"`.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Me8nvZbRIbw",
    "outputId": "c9706c16-33ec-40b3-a50e-e131f740854b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PER': 0.015, 'ORG': 0.010000000000000002, 'LOC': 0.009, 'O': 0.006999999999999999}\n"
     ]
    }
   ],
   "source": [
    "def calculate_probabilities(token, previous_tag, emission_probs, transition_probs):\n",
    "    probabilities = {}\n",
    "\n",
    "    for current_tag in transition_probs.get(previous_tag, {}):\n",
    "        emission_prob = emission_probs.get(current_tag, {}).get(token, 0)\n",
    "        transition_prob = transition_probs[previous_tag][current_tag]\n",
    "        joint_prob = emission_prob * transition_prob\n",
    "        probabilities[current_tag] = joint_prob\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "emission_probs = {\n",
    "    'PER': {'apple': 0.1, 'orange': 0.2},\n",
    "    'ORG': {'apple': 0.05, 'orange': 0.15},\n",
    "    'LOC': {'apple': 0.03, 'orange': 0.08},\n",
    "    'O': {'apple': 0.02, 'orange': 0.1}\n",
    "}\n",
    "\n",
    "transition_probs = {\n",
    "    'START': {'PER': 0.3, 'ORG': 0.2, 'LOC': 0.1, 'O': 0.4},\n",
    "    'PER': {'PER': 0.2, 'ORG': 0.3, 'LOC': 0.1, 'O': 0.4},\n",
    "    'ORG': {'PER': 0.1, 'ORG': 0.4, 'LOC': 0.2, 'O': 0.3},\n",
    "    'LOC': {'PER': 0.15, 'ORG': 0.2, 'LOC': 0.3, 'O': 0.35},\n",
    "    'O': {'PER': 0.25, 'ORG': 0.15, 'LOC': 0.1, 'O': 0.5}\n",
    "}\n",
    "\n",
    "\n",
    "current_token = \"apple\"\n",
    "previous_tag = \"LOC\"\n",
    "\n",
    "result = calculate_probabilities(current_token, previous_tag, emission_probs, transition_probs)\n",
    "\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
