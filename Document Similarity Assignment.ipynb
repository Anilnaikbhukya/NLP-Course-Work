{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M3r84BT5WeN"
   },
   "source": [
    "#955G5: Applied Natural Language Processing\n",
    "\n",
    "##Computer Based Examination, 2023\n",
    "\n",
    "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "259FEqEb5xRr"
   },
   "source": [
    "#Question 1 (50 Marks)\n",
    "\n",
    "This question is about document similarity and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89Wx4WPa5weU",
    "outputId": "6379715a-6b1a-4a37-d82c-79f3a52eee20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aniln\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aniln\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from math import log2, sqrt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "# This is corpus of quotations, their sources and languages which we will use in this question.\n",
    "corpus = [(\"All the world’s a stage, and all the men and women merely players.\", \"William Shakespeare\", \"English\"),\n",
    "          (\"Ask not what your country can do for you; ask what you can do for your country.\", \"John Kennedy\", \"English\"),\n",
    "          (\"Ask, and it shall be given you; seek, and you shall find.\", \"the Bible\", \"Greek\"),\n",
    "          (\"Eighty percent of success is showing up.\", \"Woody Allen\", \"English\"),\n",
    "          (\"Elementary, my dear Watson.\", \"Sherlock Holmes (character)\", \"English\"),\n",
    "          (\"For those to whom much is given, much is required.\", \"the Bible\", \"Greek\"),\n",
    "          (\"Frankly, my dear, I don't give a damn.\", \"Rhett Butler (character)\", \"English\"),\n",
    "          (\"Genius is one percent inspiration and ninety-nine percent perspiration.\", \"Thomas Edison\", \"English\")]\n",
    "\n",
    "# This is a query that we will retrieve relevant documents for.\n",
    "query = \"Will I be given what I ask?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee37cbR8ebN9"
   },
   "source": [
    "a) Preprocess the information in the list `corpus` by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGruck7ketTb"
   },
   "source": [
    "i) Create three dictionaries - `quotes`, `sources` and `languages` - corresponding to the three elements in each tuple of `corpus`. Each key in these dictionaries should be the position in the original list and each value should be a string from the tuple at that position in the list.\n",
    "\n",
    "Each value in `quotes` should be item 0 in a tuple, i.e. the quotation itself. Each value in `sources` should be item 1 in a tuple, i.e. the source associated with the quotation. Each value in `languages` should be item 2 in a tuple, i.e. the original language of the quotation.\n",
    "\n",
    "So for example, the corpus `[(\"Hello\", \"Alice\", \"English\")]` should be broken into three dictionaries: `{0: \"Hello\"}`, `{0: \"Alice\"}` and `{0: \"English\"}`.\n",
    "\n",
    "(7 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes: {0: 'All the world’s a stage, and all the men and women merely players.', 1: 'Ask not what your country can do for you; ask what you can do for your country.', 2: 'Ask, and it shall be given you; seek, and you shall find.', 3: 'Eighty percent of success is showing up.', 4: 'Elementary, my dear Watson.', 5: 'For those to whom much is given, much is required.', 6: \"Frankly, my dear, I don't give a damn.\", 7: 'Genius is one percent inspiration and ninety-nine percent perspiration.'}\n",
      "Sources: {0: 'William Shakespeare', 1: 'John Kennedy', 2: 'the Bible', 3: 'Woody Allen', 4: 'Sherlock Holmes (character)', 5: 'the Bible', 6: 'Rhett Butler (character)', 7: 'Thomas Edison'}\n",
      "Languages: {0: 'English', 1: 'English', 2: 'Greek', 3: 'English', 4: 'English', 5: 'Greek', 6: 'English', 7: 'English'}\n"
     ]
    }
   ],
   "source": [
    "quotes = {}\n",
    "sources = {}\n",
    "languages = {}\n",
    "\n",
    "for i, item in enumerate(corpus):\n",
    "    quotes[i] = item[0]\n",
    "    sources[i] = item[1]\n",
    "    languages[i] = item[2]\n",
    "\n",
    "    \n",
    "print(\"Quotes:\", quotes)\n",
    "print(\"Sources:\", sources)\n",
    "print(\"Languages:\", languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HKbt1Npi6U_"
   },
   "source": [
    "ii) Tokenise the quotation strings in the dictionary `quotes` to produce a new dictionary with the same keys called `tokenised`, in which each value is a list of tokens.\n",
    "\n",
    "So, for example the dictionary `{0: \"Hello Bob\"}` would become `{0: [\"Hello\", \"Bob\"]}`.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised: {0: ['All', 'the', 'world', '’', 's', 'a', 'stage', ',', 'and', 'all', 'the', 'men', 'and', 'women', 'merely', 'players', '.'], 1: ['Ask', 'not', 'what', 'your', 'country', 'can', 'do', 'for', 'you', ';', 'ask', 'what', 'you', 'can', 'do', 'for', 'your', 'country', '.'], 2: ['Ask', ',', 'and', 'it', 'shall', 'be', 'given', 'you', ';', 'seek', ',', 'and', 'you', 'shall', 'find', '.'], 3: ['Eighty', 'percent', 'of', 'success', 'is', 'showing', 'up', '.'], 4: ['Elementary', ',', 'my', 'dear', 'Watson', '.'], 5: ['For', 'those', 'to', 'whom', 'much', 'is', 'given', ',', 'much', 'is', 'required', '.'], 6: ['Frankly', ',', 'my', 'dear', ',', 'I', 'do', \"n't\", 'give', 'a', 'damn', '.'], 7: ['Genius', 'is', 'one', 'percent', 'inspiration', 'and', 'ninety-nine', 'percent', 'perspiration', '.']}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenised = {}\n",
    "for key, quote in quotes.items():\n",
    "    tokenised[key] = word_tokenize(quote)\n",
    "\n",
    "print(\"Tokenised:\", tokenised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMa4g0kGjOze"
   },
   "source": [
    "iii) Case normalise the tokenised strings and remove stopwords and punctuation, putting the results into a new dictionary called `normalised`, with the same keys as `tokenised` and the values being normalised lists of tokens.\n",
    "\n",
    "(6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised: {0: ['world', '’', 'stage', 'men', 'women', 'merely', 'players'], 1: ['ask', 'country', 'ask', 'country'], 2: ['ask', 'shall', 'given', 'seek', 'shall', 'find'], 3: ['eighty', 'percent', 'success', 'showing'], 4: ['elementary', 'dear', 'watson'], 5: ['much', 'given', 'much', 'required'], 6: ['frankly', 'dear', \"n't\", 'give', 'damn'], 7: ['genius', 'one', 'percent', 'inspiration', 'ninety-nine', 'percent', 'perspiration']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aniln\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "normalised = {}\n",
    "\n",
    "for key, tokens in tokenised.items():\n",
    "    normalized_tokens = []\n",
    "    for word in tokens:\n",
    "        lowercase_word = word.lower()\n",
    "        if lowercase_word not in stop and word not in punctuation:\n",
    "            normalized_tokens.append(lowercase_word)\n",
    "    normalised[key] = normalized_tokens\n",
    "    \n",
    "print(\"Normalised:\", normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFcNSwE-mmdt"
   },
   "source": [
    "iv) Describe two other forms of pre-processing that could be applied to text documents.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9iw4TowmvAQ"
   },
   "source": [
    "Here are two other forms of pre-processing that could be applied to text documents:\n",
    "\n",
    "1. **Lemmatization:**\n",
    "   + Lemmatization reduces words to their base or root form, known as the lemma, by removing inflections and variations.\n",
    "   + It helps with word standardization, treating different inflections of a word as the same, and, ultimately, reducing the dimensionality of text data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **Stemming:**\n",
    "   + Stemming is a more aggressive text pre-processing technique that removes suffixes from words to obtain the root or stem of the word.\n",
    "   + While stemming can produce incorrect words, it is less computationally expensive than lemmatization and is commonly used in tasks such as information retrieval to match word stems.\n",
    "\n",
    "Both lemmatization and stemming aim to simplify word representations, reduce text dimensionality, and improve natural language processing task efficiency. The technique used is determined by the requirements of the task as well as the desired balance of precision and computational efficiency.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzmBCqijjpj0"
   },
   "source": [
    "b) Convert each document, ie each list of tokens, into a tfidf representation by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c49faFMRdG0W"
   },
   "source": [
    "i) Calculate document frequencies for each token found in the documents contained in `normalised` and put the results in a dictionary called `df`. That is count how many different entries in the dictionary each token is found in.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo96NEz6Hcz8",
    "outputId": "a1ec9835-79a4-423c-974f-816e2d1103ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Frequencies (df): {'merely': 1, '’': 1, 'stage': 1, 'women': 1, 'players': 1, 'world': 1, 'men': 1, 'country': 1, 'ask': 2, 'seek': 1, 'find': 1, 'shall': 1, 'given': 2, 'showing': 1, 'eighty': 1, 'success': 1, 'percent': 2, 'elementary': 1, 'dear': 2, 'watson': 1, 'much': 1, 'required': 1, 'give': 1, \"n't\": 1, 'frankly': 1, 'damn': 1, 'inspiration': 1, 'perspiration': 1, 'one': 1, 'genius': 1, 'ninety-nine': 1}\n"
     ]
    }
   ],
   "source": [
    "df = {}\n",
    "\n",
    "for tokens in normalised.values():\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        df[token] = df.get(token, 0) + 1\n",
    "\n",
    "\n",
    "print(\"Document Frequencies (df):\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67kNG26edztz"
   },
   "source": [
    "ii) Calculate inverse document frequencies from the document frequencies derived in the previous question and put the results in a dictionary called `idf`.\n",
    "\n",
    "$$IDF(w) = \\log_2 \\left( \\frac{N}{DF(w)} \\right) $$\n",
    "\n",
    "where $N$ is the total number of documents and $DF(w)$ is the document frequency of the word $w$.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqTvv8c_wK0T",
    "outputId": "75b31933-49e4-4408-d62f-33cc3cb3294b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Document Frequencies (idf): {'merely': 3.0, '’': 3.0, 'stage': 3.0, 'women': 3.0, 'players': 3.0, 'world': 3.0, 'men': 3.0, 'country': 3.0, 'ask': 2.0, 'seek': 3.0, 'find': 3.0, 'shall': 3.0, 'given': 2.0, 'showing': 3.0, 'eighty': 3.0, 'success': 3.0, 'percent': 2.0, 'elementary': 3.0, 'dear': 2.0, 'watson': 3.0, 'much': 3.0, 'required': 3.0, 'give': 3.0, \"n't\": 3.0, 'frankly': 3.0, 'damn': 3.0, 'inspiration': 3.0, 'perspiration': 3.0, 'one': 3.0, 'genius': 3.0, 'ninety-nine': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "total_documents = len(normalised)\n",
    "idf = {word: log2(total_documents / df[word]) for word in df}\n",
    "\n",
    "\n",
    "print(\"Inverse Document Frequencies (idf):\", idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbTHONWreDTN"
   },
   "source": [
    "iii) Convert each document in `normalised` from a list of tokens to a dictionary of term frequencies and put the results in a dictionary called `tf`.\n",
    "\n",
    "The keys of `tf` should be positions in the original list `corpus` and the values should be the term frequency dictionaries, which map from tokens to frequency in each document.\n",
    "\n",
    "So, for example, `{0: [\"Hello\", \"Bob\"]}` would become `{0: {\"Hello\": 1, \"Bob\": 1}}`.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5cfBQOTw7i_",
    "outputId": "5e08747b-2eea-4aa8-bac1-b3e67b1487a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequencies (tf): {0: {'world': 1, '’': 1, 'stage': 1, 'men': 1, 'women': 1, 'merely': 1, 'players': 1}, 1: {'ask': 2, 'country': 2}, 2: {'ask': 1, 'shall': 2, 'given': 1, 'seek': 1, 'find': 1}, 3: {'eighty': 1, 'percent': 1, 'success': 1, 'showing': 1}, 4: {'elementary': 1, 'dear': 1, 'watson': 1}, 5: {'much': 2, 'given': 1, 'required': 1}, 6: {'frankly': 1, 'dear': 1, \"n't\": 1, 'give': 1, 'damn': 1}, 7: {'genius': 1, 'one': 1, 'percent': 2, 'inspiration': 1, 'ninety-nine': 1, 'perspiration': 1}}\n"
     ]
    }
   ],
   "source": [
    "tf = {}\n",
    "\n",
    "for position, tokens in normalised.items():\n",
    "    term_frequency = {}\n",
    "    for token in tokens:\n",
    "        term_frequency[token] = term_frequency.get(token, 0) + 1\n",
    "    tf[position] = term_frequency\n",
    "\n",
    "\n",
    "print(\"Term Frequencies (tf):\", tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSvH-9m9fWdV"
   },
   "source": [
    "iv) Convert the raw term frequencies in `tf` to tfidf values using the dictionary `idf`.\n",
    "\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Values: {0: {'world': 2.807354922057604, 'stage': 2.807354922057604, 'men': 2.807354922057604, 'women': 2.807354922057604, 'merely': 2.807354922057604, 'players': 2.807354922057604}, 1: {'ask': 3.1699250014423126, 'country': 5.614709844115208}, 2: {'ask': 1.5849625007211563, 'given': 2.807354922057604, 'seek': 2.807354922057604, 'find': 2.807354922057604}, 3: {'eighty': 2.807354922057604, 'percent': 2.0, 'success': 2.807354922057604, 'showing': 2.807354922057604}, 4: {'elementary': 2.807354922057604, 'dear': 3.1699250014423126, 'watson': 2.807354922057604}, 5: {'much': 2.0, 'given': 2.807354922057604, 'required': 2.807354922057604}, 6: {'frankly': 2.807354922057604, 'dear': 1.5849625007211563, \"n't\": 2.807354922057604, 'give': 2.807354922057604, 'damn': 2.807354922057604}, 7: {'genius': 2.807354922057604, 'one': 2.807354922057604, 'percent': 2.0, 'inspiration': 2.807354922057604, 'ninety-nine': 2.807354922057604, 'perspiration': 2.807354922057604}}\n"
     ]
    }
   ],
   "source": [
    "tf = {\n",
    "    0: {\"world\": 1, \"stage\": 1, \"men\": 1, \"women\": 1, \"merely\": 1, \"players\": 1},\n",
    "    1: {\"ask\": 2, \"country\": 2},\n",
    "    2: {\"ask\": 1, \"given\": 1, \"seek\": 1, \"find\": 1},\n",
    "    3: {\"eighty\": 1, \"percent\": 2, \"success\": 1, \"showing\": 1},\n",
    "    4: {\"elementary\": 1, \"dear\": 2, \"watson\": 1},\n",
    "    5: {\"much\": 2, \"given\": 1, \"required\": 1},\n",
    "    6: {\"frankly\": 1, \"dear\": 1, \"n't\": 1, \"give\": 1, \"damn\": 1},\n",
    "    7: {\"genius\": 1, \"one\": 1, \"percent\": 2, \"inspiration\": 1, \"ninety-nine\": 1, \"perspiration\": 1}\n",
    "}\n",
    "\n",
    "idf = {\n",
    "    \"world\": 2.807354922057604,\n",
    "    \"stage\": 2.807354922057604,\n",
    "    \"men\": 2.807354922057604,\n",
    "    \"women\": 2.807354922057604,\n",
    "    \"merely\": 2.807354922057604,\n",
    "    \"players\": 2.807354922057604,\n",
    "    \"ask\": 1.5849625007211563,\n",
    "    \"country\": 2.807354922057604,\n",
    "    \"given\": 2.807354922057604,\n",
    "    \"seek\": 2.807354922057604,\n",
    "    \"find\": 2.807354922057604,\n",
    "    \"eighty\": 2.807354922057604,\n",
    "    \"percent\": 1.0,\n",
    "    \"success\": 2.807354922057604,\n",
    "    \"showing\": 2.807354922057604,\n",
    "    \"elementary\": 2.807354922057604,\n",
    "    \"dear\": 1.5849625007211563,\n",
    "    \"watson\": 2.807354922057604,\n",
    "    \"much\": 1.0,\n",
    "    \"required\": 2.807354922057604,\n",
    "    \"frankly\": 2.807354922057604,\n",
    "    \"n't\": 2.807354922057604,\n",
    "    \"give\": 2.807354922057604,\n",
    "    \"damn\": 2.807354922057604,\n",
    "    \"genius\": 2.807354922057604,\n",
    "    \"one\": 2.807354922057604,\n",
    "    \"inspiration\": 2.807354922057604,\n",
    "    \"ninety-nine\": 2.807354922057604,\n",
    "    \"perspiration\": 2.807354922057604\n",
    "}\n",
    "\n",
    "\n",
    "tfidf = {}\n",
    "\n",
    "for position, term_frequency in tf.items():\n",
    "    tfidf[position] = {term: freq * idf[term] for term, freq in term_frequency.items()}\n",
    "\n",
    "\n",
    "print(\"TF-IDF Values:\", tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKSzAq1zfj-L"
   },
   "source": [
    "c) In the following steps, preprocess the string `query` and convert it to a tfidf representation then use this to find relevant quotations in the index `tfidf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GF8dFWsHWlr"
   },
   "source": [
    "i) Define a function `dot` which takes two dictionaries containing tfidf values as inputs and calculates their dot product.\n",
    "\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ChDKHmrJFAiz"
   },
   "outputs": [],
   "source": [
    "def dot(tfidf1, tfidf2):\n",
    "    common_terms = set(tfidf1.keys()) & set(tfidf2.keys())\n",
    "    dot_product = sum(tfidf1[term] * tfidf2[term] for term in common_terms)\n",
    "    print(\"Dot Product:\", dot_product)\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS6SEhUEHpsm"
   },
   "source": [
    "ii) Define a function, `sim`, which takes two dictionaries containing tfidf values as inputs and , using your `dot` function, calculates their cosine similarity.\n",
    "\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9kL3ufDkFOuX"
   },
   "outputs": [],
   "source": [
    "def sim(tfidf1, tfidf2):\n",
    "    dot_product = dot(tfidf1, tfidf2)\n",
    "    magnitude1 = sqrt(sum(value ** 2 for value in tfidf1.values()))\n",
    "    magnitude2 = sqrt(sum(value ** 2 for value in tfidf2.values()))\n",
    "\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        similarity = 0.0\n",
    "    else:\n",
    "        similarity = dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "    print(\"Cosine Similarity:\", similarity)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x17i627INSk"
   },
   "source": [
    "iii) Preprocess the string `query` and convert it to a tfidf representation. Then calculate its cosine similarity to all the documents in the dictionary `tfidf`.\n",
    "\n",
    "For any document with a non-zero similarity, print out the similarity, source and language of the original quotation.\n",
    "\n",
    "(8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Mf_pHqq4kzk",
    "outputId": "7053454c-681e-4b8a-c423-b4cbc1c0b7b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 0\n",
      "Cosine Similarity: 0.0\n",
      "Dot Product: 5.024212257384523\n",
      "Cosine Similarity: 0.24170326829742467\n",
      "Similarity with Document 1: 0.2417, Source: John Kennedy, Language: English\n",
      "Dot Product: 10.393347787093319\n",
      "Cosine Similarity: 0.6303669975629038\n",
      "Similarity with Document 2: 0.6304, Source: the Bible, Language: Greek\n",
      "Dot Product: 0\n",
      "Cosine Similarity: 0.0\n",
      "Dot Product: 0\n",
      "Cosine Similarity: 0.0\n",
      "Dot Product: 7.8812416584010565\n",
      "Cosine Similarity: 0.5499157470489209\n",
      "Similarity with Document 5: 0.5499, Source: the Bible, Language: Greek\n",
      "Dot Product: 0\n",
      "Cosine Similarity: 0.0\n",
      "Dot Product: 0\n",
      "Cosine Similarity: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from math import sqrt\n",
    "\n",
    "query = \"Will I be given what I ask?\"\n",
    "\n",
    "\n",
    "query_tokens = [word.lower() for word in word_tokenize(query) if word.lower() not in stop and word not in punctuation]\n",
    "query_tfidf = {term: query_tokens.count(term) * idf[term] for term in query_tokens}\n",
    "\n",
    "\n",
    "for position, document_tfidf in tfidf.items():\n",
    "    similarity = sim(query_tfidf, document_tfidf)\n",
    "    if similarity != 0:\n",
    "        source = sources[position]\n",
    "        language = languages[position]\n",
    "        print(f\"Similarity with Document {position}: {similarity:.4f}, Source: {source}, Language: {language}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
